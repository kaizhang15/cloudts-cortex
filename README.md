## CloudTS Cortex Integration

This work implementation is based on Cortex v1.16.0 and Prometheus v2.48.0.



### Get Start

To get start to work with CloudTS, it is recommended to first run pure Cortex, Prometheus and Node Exporter to say "hello world".

For Cortex and Prometheus, please find the source code from github ([Cortex v1.16.0](https://github.com/cortexproject/cortex/tree/v1.16.0), [Prometheus v2.48.0](https://github.com/prometheus/prometheus/tree/v2.48.0).

For Node Exporter, please install it with release package [Node Exporter](https://github.com/prometheus/node_exporter/releases?q=1.7.0&expanded=true).

To build these source code, you need:

- Go [version 1.22 or greater](https://golang.org/doc/install).
- NodeJS [version 16 or greater](https://nodejs.org/).
- npm [version 7 or greater](https://www.npmjs.com/).

For Node Exporter, please run it as follows:

```
tar -xzf node_exporter-1.7.0.linux-amd64.tar.gz
cd node_exporter-1.7.0.linux-amd64
cp node_exporter-1.7.0.linux-amd64/node_exporter /usr/local/bin/
node_exporter
```

For Prometheus, please build and run it as follows:

```
cd prometheus (github clone directory)
make build
./prometheus --config.file=./documentation/examples/prometheus.yml
```

For Cortex, please build it as follows:

```
cd cortex (github clone directory)
go build ./cmd/cortex
./cortex -config.file=./docs/configuration/single-process-config.yaml
```

To collect data from Node Exporter, you need to modify the configuration files of Prometheus.

For prometheus.yml, add following under `scrape_configs` and `remote_write`

```
scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
  # Collect node exporter data
  - job_name: 'node'
    static_configs:
      - targets: ['localhost:9100']
remote_write:
- url: http://localhost:9009/api/prom/push
```

Then you may find the monitoring data is collected by Prometheus an Cortex.



To run CloudTS with cortex, it has the same build steps.

For CloudTS, it has more configuration options as listed below:

```
cloudts:
  s3:
    bucket: my-bucket
    region: region_exp
    access_key: xxxxxxx
    secret_key: xxxxxxx
    endpoint: s3-endpoint
  partition_duration: 2h
  max_upload_connections: 5
  
```




### Evaluation

The evaluation is based on EC2 m5.2xlarge server, with Ubuntu 22.04 LTS and S3 storage.



#### Production Environment

For the production environment evaluation, you can run it as follows: 

1. To collect data, you can locally run Prometheus and multiple Node Exporters with flag `--web.listen-address=:9200` to indicate unique port. In prometheus.yml under `scrape_configs`, add more jobs.

   ```
   scrape_configs:
     - job_name: 'prometheus'
       static_configs:
         - targets: ['localhost:9090']
     # Collect node exporter data
     - job_name: 'node1'
       static_configs:
         - targets: ['localhost:9100']
     - job_name: 'node2'
       static_configs:
         - targets: ['localhost:9200']  
     # Add more node exporter jobs as you need
   remote_write:
   - url: http://localhost:9009/api/prom/push
   ```

2. Run Cortex with S3 storage backend enabled and CloudTS simultaneously.

   ```
   # cortex_s3.yaml
   
   blocks_storage:
     tsdb:
       dir: ./data/cortex/tsdb
   
     bucket_store:
       sync_dir: ./data/cortex/tsdb-sync
   
     # You can choose between local storage and Amazon S3, Google GCS and Azure storage. Each option requires additional configuration
     # as shown below. All options can be configured via flags as well which might be handy for secret inputs.
     backend: s3 # s3, gcs, azure or filesystem are valid options
     s3:
       bucket: my-bucket
       endpoint: s3-endpoint
       # Configure your S3 credentials below.
       secret_access_key: "xxxxxxxxxxxx"
       access_key_id:     "xxxxxxxxxxxx"
   ```

   ```
   # cortex_cloudts.yaml
   
   cloudts:
     s3:
       bucket: my-bucket
       region: region_exp
       access_key: xxxxxxx
       secret_key: xxxxxxx
       endpoint: s3-endpoint
     partition_duration: 2h
     max_upload_connections: 5
     
   ```

3. After enough data collected by Cortex and CloudTS, query data on S3 with http api provided by Cortex (https://cortexmetrics.io/docs/api/#range-query).



#### Synthetic Workload

For the synthetic workload, more node exporters should be utilized to generate data (50 node exporters on 10 virtual machines in the paper, similar as previous described, you can simply change the port information in a single machine and update Prometheus.yml to collect data), and we follow the query patterns generated by [TSBS](https://github.com/timescale/tsbs) as described in the paper to evaluate the performance. 

To evaluate it, you can run as follows:

1. To collect data, you can locally run Prometheus and multiple Node Exporters first.

2. Run Cortex with S3 storage backend enabled and CloudTS simultaneously with the same configuration as shown above.

3. For the queries, you can install TSBS, utilize it to generate corresponding queries with "victoriametrics" format, and finally change the queries into [cortex format](https://cortexmetrics.io/docs/api/#range-query) manually.

   ```
   # To install TSBS
   git clone https://github.com/timescale/tsbs
   cd tsbs
   make
   
   # Utilize TSBS to generate queries
   cd $YOUR_TSBS_Directory$/bin
   	# '--query-type' indicates query pattern; '--scale indicates' the number of node exporters; '--queries' indicates different query numbers
   ./tsbs_generate_queries --use-case="devops" --seed=123 --scale=4000     --timestamp-start="2016-01-01T00:00:00Z"     --timestamp-end="2016-01-04T00:00:01Z"     --queries=1000 --query-type="single-groupby-1-8-1" --format="victoriametrics" > queries.txt
   ```

   

### Reproducing

Scripts are provided in the `./scripts` directory to facilitate the reproduction of main results. Follow the detailed instructions provided in the [scripts directory](https://github.com/kaizhang15/cloudts-cortex/blob/main/scripts/README.md).

